# -*- coding: utf-8 -*-
"""
GMP ì˜ë¯¸ ê¸°ë°˜ ì¬ì²­í‚¹ ë„êµ¬ (FDA í•„í„°ë§ ë²„ì „)
ê¸°ì¡´ SemanticChunkerë¡œ ìƒì„±ëœ ì²­í¬ë¥¼ ë” ì‘ì€ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì¬ì²­í‚¹

ì‹¤í–‰ ì½”ë“œ:
python semantic_rechunk.py --input fda_semantic_chunks.jsonl --output fda_2nd_semantic_chunks.jsonl --filter FDA
"""

import argparse
import json
import os
import time
from pathlib import Path
from typing import List, Optional, Dict, Any
from dataclasses import dataclass, asdict
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed

# LangChain imports
from langchain_experimental.text_splitter import SemanticChunker
from langchain.embeddings import HuggingFaceEmbeddings

@dataclass
class ChunkRecord:
    """ì²­í¬ ë ˆì½”ë“œ êµ¬ì¡°"""
    id: str
    doc_id: str
    source_path: str
    title: str
    jurisdiction: Optional[str]
    doc_date: Optional[str] 
    doc_version: Optional[str]
    section_id: Optional[str]
    section_title: Optional[str]
    page_start: int
    page_end: int
    chunk_index: int
    text: str
    parent_chunk_id: Optional[str] = None  # ì›ë³¸ ì²­í¬ ID
    sub_chunk_index: Optional[int] = None   # ì„œë¸Œ ì²­í¬ ì¸ë±ìŠ¤
    semantic_level: Optional[int] = None    # ì˜ë¯¸ ë¶„í•  ë ˆë²¨

class SemanticRechunker:
    """ì˜ë¯¸ ê¸°ë°˜ ì¬ì²­í‚¹ê¸°"""
    
    def __init__(self, 
                 model_name: str = "jhgan/ko-sroberta-multitask",
                 device: str = 'cpu',
                 breakpoint_threshold_type: str = "percentile",
                 breakpoint_threshold_amount: int = 50,
                 min_chunk_length: int = 100):
        self.model_name = model_name
        self.device = device
        self.breakpoint_threshold_type = breakpoint_threshold_type
        self.breakpoint_threshold_amount = breakpoint_threshold_amount
        self.min_chunk_length = min_chunk_length
        
        # ì§€ì—° ì´ˆê¸°í™”
        self._embeddings = None
        self._splitter = None
    
    @property
    def embeddings(self):
        if self._embeddings is None:
            print(f"[PID {os.getpid()}] Loading embeddings model: {self.model_name}")
            self._embeddings = HuggingFaceEmbeddings(
                model_name=self.model_name,
                encode_kwargs={'normalize_embeddings': False},
                model_kwargs={'device': self.device},
            )
        return self._embeddings
    
    @property
    def splitter(self):
        if self._splitter is None:
            print(f"[PID {os.getpid()}] Initializing SemanticChunker")
            print(f"[PID {os.getpid()}] Threshold: {self.breakpoint_threshold_type} = {self.breakpoint_threshold_amount}")
            
            self._splitter = SemanticChunker(
                self.embeddings,
                breakpoint_threshold_type=self.breakpoint_threshold_type,
                breakpoint_threshold_amount=self.breakpoint_threshold_amount,
            )
        return self._splitter

# ì „ì—­ ì¬ì²­í‚¹ê¸° ë³€ìˆ˜
_global_semantic_rechunker = None

def init_semantic_worker(model_name, device, breakpoint_threshold_type, 
                        breakpoint_threshold_amount, min_chunk_length):
    """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì´ˆê¸°í™”"""
    global _global_semantic_rechunker
    _global_semantic_rechunker = SemanticRechunker(
        model_name=model_name,
        device=device,
        breakpoint_threshold_type=breakpoint_threshold_type,
        breakpoint_threshold_amount=breakpoint_threshold_amount,
        min_chunk_length=min_chunk_length
    )

def semantic_rechunk_single_record(chunk_record_dict: Dict[str, Any]) -> List[ChunkRecord]:
    """ë‹¨ì¼ ì²­í¬ë¥¼ ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ì¬ì²­í‚¹"""
    global _global_semantic_rechunker
    
    try:
        # ì›ë³¸ ì²­í¬ ë ˆì½”ë“œ ìƒì„±
        original_record = ChunkRecord(**chunk_record_dict)
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì¬ì²­í‚¹ ì•ˆí•¨
        if len(original_record.text) <= _global_semantic_rechunker.min_chunk_length:
            original_record.parent_chunk_id = original_record.id
            original_record.sub_chunk_index = 0
            original_record.semantic_level = 1  # 1ì°¨ ì˜ë¯¸ ë ˆë²¨
            return [original_record]
        
        # ì˜ë¯¸ ê¸°ë°˜ ì¬ì²­í‚¹ ìˆ˜í–‰
        try:
            sub_chunks = _global_semantic_rechunker.splitter.split_text(original_record.text)
        except Exception as e:
            print(f"[PID {os.getpid()}] SemanticChunker error: {e}, falling back to original")
            original_record.parent_chunk_id = original_record.id
            original_record.sub_chunk_index = 0
            original_record.semantic_level = 1
            return [original_record]
        
        # ì„œë¸Œì²­í¬ê°€ 1ê°œì´ê±°ë‚˜ ì›ë³¸ê³¼ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        if len(sub_chunks) <= 1:
            original_record.parent_chunk_id = original_record.id
            original_record.sub_chunk_index = 0
            original_record.semantic_level = 1
            return [original_record]
        
        # ì¬ì²­í‚¹ íš¨ê³¼ê°€ ë¯¸ë¯¸í•˜ë©´ ì›ë³¸ ë°˜í™˜ (í‰ê·  ê¸¸ì´ ì°¨ì´ê°€ 20% ì´í•˜)
        original_length = len(original_record.text)
        avg_sub_length = sum(len(chunk) for chunk in sub_chunks) / len(sub_chunks)
        
        if avg_sub_length > original_length * 0.8:  # 80% ì´ìƒì´ë©´ ì˜ë¯¸ìˆëŠ” ë¶„í• ì´ ì•„ë‹˜
            original_record.parent_chunk_id = original_record.id
            original_record.sub_chunk_index = 0
            original_record.semantic_level = 1
            return [original_record]
        
        # ìƒˆë¡œìš´ ì˜ë¯¸ ê¸°ë°˜ ì²­í¬ ë ˆì½”ë“œë“¤ ìƒì„±
        new_records = []
        for sub_idx, sub_text in enumerate(sub_chunks):
            # ë„ˆë¬´ ì§§ì€ ì„œë¸Œì²­í¬ëŠ” ì œì™¸
            if len(sub_text.strip()) < 50:
                continue
                
            new_record = ChunkRecord(
                id=f"{original_record.id}-sem{sub_idx:03d}",
                doc_id=original_record.doc_id,
                source_path=original_record.source_path,
                title=original_record.title,
                jurisdiction=original_record.jurisdiction,
                doc_date=original_record.doc_date,
                doc_version=original_record.doc_version,
                section_id=original_record.section_id,
                section_title=original_record.section_title,
                page_start=original_record.page_start,
                page_end=original_record.page_end,
                chunk_index=original_record.chunk_index,
                text=sub_text.strip(),
                parent_chunk_id=original_record.id,
                sub_chunk_index=sub_idx,
                semantic_level=2  # 2ì°¨ ì˜ë¯¸ ë ˆë²¨
            )
            new_records.append(new_record)
        
        # ìœ íš¨í•œ ì„œë¸Œì²­í¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        if not new_records:
            original_record.parent_chunk_id = original_record.id
            original_record.sub_chunk_index = 0
            original_record.semantic_level = 1
            return [original_record]
        
        return new_records
        
    except Exception as e:
        print(f"[PID {os.getpid()}] Error in semantic rechunking: {e}")
        # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°˜í™˜
        original_record = ChunkRecord(**chunk_record_dict)
        original_record.parent_chunk_id = original_record.id
        original_record.sub_chunk_index = 0
        original_record.semantic_level = 1
        return [original_record]

def load_jsonl_chunks(jsonl_path: Path) -> List[Dict[str, Any]]:
    """JSONL íŒŒì¼ì—ì„œ ì²­í¬ ë¡œë“œ"""
    print(f"ğŸ“– Loading chunks from: {jsonl_path}")
    
    chunks = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                chunk_data = json.loads(line.strip())
                chunks.append(chunk_data)
            except json.JSONDecodeError as e:
                print(f"âŒ JSON parsing error at line {line_num}: {e}")
                continue
    
    print(f"âœ… Loaded {len(chunks)} chunks")
    return chunks

def filter_chunks_by_jurisdiction(chunks: List[Dict[str, Any]], target_filter: str = None) -> List[Dict[str, Any]]:
    """ê´€í•  ì§€ì—­ë³„ë¡œ ì²­í¬ í•„í„°ë§"""
    if not target_filter:
        log("í•„í„°ë§ ì—†ì´ ì „ì²´ ë°ì´í„° ì‚¬ìš©")
        return chunks
    
    print(f"ğŸ” '{target_filter}' ê´€í•  ì§€ì—­ìœ¼ë¡œ í•„í„°ë§ ì‹œì‘...")
    
    filtered_chunks = []
    for chunk in chunks:
        jurisdiction = chunk.get("jurisdiction", "")
        
        # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ í¬í•¨ ì—¬ë¶€ í™•ì¸
        if jurisdiction and target_filter.lower() in jurisdiction.lower():
            filtered_chunks.append(chunk)
    
    print(f"âœ… í•„í„°ë§ ê²°ê³¼: {len(filtered_chunks)}ê°œ ì²­í¬ (ì „ì²´: {len(chunks)}ê°œ)")
    
    if len(filtered_chunks) == 0:
        print(f"âš ï¸ '{target_filter}' ê´€í•  ì§€ì—­ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ê´€í•  ì§€ì—­ í‘œì‹œ
        unique_jurisdictions = set()
        for chunk in chunks:
            if chunk.get("jurisdiction"):
                unique_jurisdictions.add(chunk["jurisdiction"])
        
        if unique_jurisdictions:
            print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ê´€í•  ì§€ì—­: {sorted(unique_jurisdictions)}")
    
    return filtered_chunks

def save_jsonl_chunks(records: List[ChunkRecord], output_path: Path) -> None:
    """ì²­í¬ ë ˆì½”ë“œë“¤ì„ JSONL íŒŒì¼ë¡œ ì €ì¥"""
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for record in records:
            record_dict = asdict(record)
            f.write(json.dumps(record_dict, ensure_ascii=False) + '\n')
    
    print(f"ğŸ’¾ Saved {len(records)} semantic rechunked records to: {output_path}")

def semantic_rechunk_parallel(input_chunks: List[Dict[str, Any]], 
                             model_name: str = "jhgan/ko-sroberta-multitask",
                             device: str = 'cpu',
                             breakpoint_threshold_type: str = "percentile",
                             breakpoint_threshold_amount: int = 50,
                             min_chunk_length: int = 100,
                             max_workers: int = None) -> List[ChunkRecord]:
    """ë³‘ë ¬ë¡œ ì˜ë¯¸ ê¸°ë°˜ ì¬ì²­í‚¹"""
    
    if max_workers is None:
        max_workers = min(mp.cpu_count() // 2, len(input_chunks), 4)
    
    print(f"ğŸ”„ Semantic rechunking {len(input_chunks)} chunks with {max_workers} workers")
    print(f"ğŸ¤– Model: {model_name}")
    print(f"ğŸ“Š Threshold: {breakpoint_threshold_type} = {breakpoint_threshold_amount}")
    print(f"ğŸ“ Min chunk length: {min_chunk_length}")
    
    start_time = time.time()
    all_records = []
    
    with ProcessPoolExecutor(
        max_workers=max_workers,
        initializer=init_semantic_worker,
        initargs=(model_name, device, breakpoint_threshold_type,
                 breakpoint_threshold_amount, min_chunk_length)
    ) as executor:
        
        # ì‘ì—… ì œì¶œ
        future_to_chunk = {
            executor.submit(semantic_rechunk_single_record, chunk_data): i
            for i, chunk_data in enumerate(input_chunks)
        }
        
        # ê²°ê³¼ ìˆ˜ì§‘
        completed_count = 0
        rechunked_count = 0
        kept_original_count = 0
        
        for future in as_completed(future_to_chunk):
            chunk_idx = future_to_chunk[future]
            try:
                records = future.result()
                all_records.extend(records)
                completed_count += 1
                
                # ì¬ì²­í‚¹ í†µê³„
                if len(records) > 1:
                    rechunked_count += 1
                else:
                    kept_original_count += 1
                
                if completed_count % 50 == 0:
                    elapsed = time.time() - start_time
                    rechunk_ratio = rechunked_count / completed_count * 100
                    print(f"ğŸ“ˆ Progress: {completed_count}/{len(input_chunks)} chunks processed")
                    print(f"   Generated: {len(all_records)} sub-chunks")
                    print(f"   Rechunked: {rechunked_count} ({rechunk_ratio:.1f}%)")
                    print(f"   Kept original: {kept_original_count}")
                    print(f"   Elapsed: {elapsed:.1f}s")
                
            except Exception as e:
                print(f"âŒ Error processing chunk {chunk_idx}: {e}")
    
    total_time = time.time() - start_time
    expansion_ratio = len(all_records) / len(input_chunks) if input_chunks else 0
    rechunk_ratio = rechunked_count / len(input_chunks) * 100 if input_chunks else 0
    
    print(f"\nâœ… Semantic rechunking completed!")
    print(f"â±ï¸  Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)")
    print(f"ğŸ“Š Original chunks: {len(input_chunks)}")
    print(f"ğŸ“Š New chunks: {len(all_records)}")
    print(f"ğŸ“Š Expansion ratio: {expansion_ratio:.2f}x")
    print(f"ğŸ“Š Rechunked ratio: {rechunk_ratio:.1f}%")
    print(f"ğŸ“Š Average processing time: {total_time/len(input_chunks):.2f}s per chunk")
    
    return all_records

def semantic_rechunk_jsonl_file(input_path: str,
                               output_path: str,
                               jurisdiction_filter: str = None,  # â­ í•„í„° ë§¤ê°œë³€ìˆ˜ ì¶”ê°€ â­
                               model_name: str = "jhgan/ko-sroberta-multitask",
                               device: str = 'cpu',
                               breakpoint_threshold_type: str = "percentile",
                               breakpoint_threshold_amount: int = 50,
                               min_chunk_length: int = 100,
                               max_workers: int = None,
                               batch_size: int = None):
    """JSONL íŒŒì¼ ì˜ë¯¸ ê¸°ë°˜ ì¬ì²­í‚¹ ë©”ì¸ í•¨ìˆ˜ (í•„í„°ë§ í¬í•¨)"""
    
    input_path = Path(input_path)
    output_path = Path(output_path)
    
    if not input_path.exists():
        print(f"âŒ Error: Input file not found: {input_path}")
        return
    
    print(f"ğŸš€ Starting semantic rechunking process")
    print(f"ğŸ“ Input: {input_path}")
    print(f"ğŸ“ Output: {output_path}")
    if jurisdiction_filter:
        print(f"ğŸ” Filter: {jurisdiction_filter}")
    
    # ì²­í¬ ë¡œë“œ
    input_chunks = load_jsonl_chunks(input_path)
    if not input_chunks:
        print("âŒ No chunks to process")
        return
    
    # â­ ê´€í•  ì§€ì—­ í•„í„°ë§ ì¶”ê°€ â­
    if jurisdiction_filter:
        filtered_chunks = filter_chunks_by_jurisdiction(input_chunks, jurisdiction_filter)
        if not filtered_chunks:
            print("âŒ í•„í„°ë§ ê²°ê³¼ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        input_chunks = filtered_chunks
    
    # ìƒ˜í”Œ ì²­í¬ ë¶„ì„
    sample_lengths = [len(chunk.get('text', '')) for chunk in input_chunks[:100]]
    avg_length = sum(sample_lengths) / len(sample_lengths)
    max_length = max(sample_lengths)
    min_length = min(sample_lengths)
    
    print(f"ğŸ“ Sample chunk analysis (first 100):")
    print(f"   Average length: {avg_length:.0f} chars")
    print(f"   Max length: {max_length} chars")
    print(f"   Min length: {min_length} chars")
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì—¬ë¶€
    if batch_size and len(input_chunks) > batch_size:
        all_records = []
        total_batches = (len(input_chunks) + batch_size - 1) // batch_size
        
        for i in range(0, len(input_chunks), batch_size):
            batch = input_chunks[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            print(f"\nğŸ”„ Processing batch {batch_num}/{total_batches} ({len(batch)} chunks)")
            
            batch_records = semantic_rechunk_parallel(
                input_chunks=batch,
                model_name=model_name,
                device=device,
                breakpoint_threshold_type=breakpoint_threshold_type,
                breakpoint_threshold_amount=breakpoint_threshold_amount,
                min_chunk_length=min_chunk_length,
                max_workers=max_workers
            )
            
            all_records.extend(batch_records)
            print(f"âœ… Batch {batch_num} completed. Total sub-chunks: {len(all_records)}")
    
    else:
        # ì „ì²´ ì²˜ë¦¬
        all_records = semantic_rechunk_parallel(
            input_chunks=input_chunks,
            model_name=model_name,
            device=device,
            breakpoint_threshold_type=breakpoint_threshold_type,
            breakpoint_threshold_amount=breakpoint_threshold_amount,
            min_chunk_length=min_chunk_length,
            max_workers=max_workers
        )
    
    # ê²°ê³¼ ì €ì¥
    save_jsonl_chunks(all_records, output_path)
    
    # ìµœì¢… í†µê³„
    semantic_level_1 = sum(1 for r in all_records if r.semantic_level == 1)
    semantic_level_2 = sum(1 for r in all_records if r.semantic_level == 2)
    
    print(f"\nğŸ‰ Semantic rechunking completed!")
    print(f"ğŸ“ Input: {input_path} ({len(input_chunks)} chunks)")
    print(f"ğŸ“ Output: {output_path} ({len(all_records)} chunks)")
    if jurisdiction_filter:
        print(f"ğŸ” Filter: {jurisdiction_filter}")
    print(f"ğŸ“Š Semantic Level 1 (original): {semantic_level_1}")
    print(f"ğŸ“Š Semantic Level 2 (rechunked): {semantic_level_2}")

def main():
    """CLI ì¸í„°í˜ì´ìŠ¤"""
    parser = argparse.ArgumentParser(description="GMP ì˜ë¯¸ ê¸°ë°˜ ì¬ì²­í‚¹ ë„êµ¬ (í•„í„°ë§ ì§€ì›)")
    
    parser.add_argument("--input", type=str, required=True, help="ì…ë ¥ JSONL íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output", type=str, required=True, help="ì¶œë ¥ JSONL íŒŒì¼ ê²½ë¡œ")
    
    # â­ í•„í„°ë§ ì˜µì…˜ ì¶”ê°€ â­
    parser.add_argument("--filter", type=str, help="ê´€í•  ì§€ì—­ í•„í„° (ì˜ˆ: FDA, EMA, KFDA)")
    
    # ì˜ë¯¸ ì²­í‚¹ íŒŒë¼ë¯¸í„°
    parser.add_argument("--model", type=str, default="jhgan/ko-sroberta-multitask", help="ì„ë² ë”© ëª¨ë¸ëª…")
    parser.add_argument("--device", type=str, default="cpu", help="ë””ë°”ì´ìŠ¤ (cpu/cuda)")
    parser.add_argument("--threshold-type", 
                       choices=["percentile", "standard_deviation", "interquartile"],
                       default="percentile", help="ì˜ë¯¸ ë¶„í•  ê¸°ì¤€")
    parser.add_argument("--threshold-amount", type=int, default=50,
                       help="ì˜ë¯¸ ë¶„í•  ì„ê³„ê°’ (ì‘ì„ìˆ˜ë¡ ë” ì„¸ë¶„í™”)")
    parser.add_argument("--min-length", type=int, default=100,
                       help="ìµœì†Œ ì²­í¬ ê¸¸ì´ (ì´ë³´ë‹¤ ì§§ìœ¼ë©´ ì¬ì²­í‚¹ ì•ˆí•¨)")
    
    # ë³‘ë ¬ ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
    parser.add_argument("--max-workers", type=int, default=None, help="ìµœëŒ€ ì›Œì»¤ ìˆ˜")
    parser.add_argument("--batch-size", type=int, default=None, help="ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì ˆì•½ìš©)")
    
    args = parser.parse_args()
    
    semantic_rechunk_jsonl_file(
        input_path=args.input,
        output_path=args.output,
        jurisdiction_filter=args.filter,  # â­ í•„í„° ë§¤ê°œë³€ìˆ˜ ì „ë‹¬ â­
        model_name=args.model,
        device=args.device,
        breakpoint_threshold_type=args.threshold_type,
        breakpoint_threshold_amount=args.threshold_amount,
        min_chunk_length=args.min_length,
        max_workers=args.max_workers,
        batch_size=args.batch_size
    )

if __name__ == "__main__":
    main()
